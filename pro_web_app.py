import streamlit as st
import os
from PIL import Image
from google import genai
from google.genai import types
from gtts import gTTS
import io
from streamlit_mic_recorder import mic_recorder

# -----------------------------------------------------------
# 1. èº«ä»½è®¤è¯ (æœä½³é¹ä¸“ç”¨)
# -----------------------------------------------------------
#
my_api_key = "AIzaSyAuaxPpzujWcarcPUZoZKsNpaF810lco4M"

if "GEMINI_API_KEY" in st.secrets:
    my_api_key = st.secrets["GEMINI_API_KEY"]

st.set_page_config(page_title="æœä½³é¹çš„ä¸‡èƒ½å®éªŒå®¤", layout="wide")
st.title("âš”ï¸ æœæ°å…¨æ ˆ AI åŠ©æ‰‹ (è”ç½‘å¢å¼ºç‰ˆ)")

# -----------------------------------------------------------
# 2. åˆå§‹åŒ– AI æ ¸å¿ƒ
# -----------------------------------------------------------
if "client" not in st.session_state:
    st.session_state.client = genai.Client(api_key=my_api_key)
    #
    models = [m.name for m in st.session_state.client.models.list() if "generateContent" in m.supported_actions]
    st.session_state.target_model = next((n for n in models if "flash" in n), models[0])

if "messages" not in st.session_state:
    st.session_state.messages = []

# -----------------------------------------------------------
# 3. ä¾§è¾¹æ é…ç½®
# -----------------------------------------------------------
with st.sidebar:
    st.header("ğŸ­ çµé­‚æ³¨å…¥")
    personality_type = st.selectbox("é€‰æ‹©çµé­‚ï¼š", ["æ­¦æ—å¤§ä¾ ", "æ¯’èˆŒç å†œ", "æ¸©æŸ”è€å¸ˆ"])
    personalities = {
        "æ­¦æ—å¤§ä¾ ": "ä½ æ˜¯ä¸€ä¸ªéšå±…æ­¦æ—å¤§ä¾ ï¼Œç§°å‘¼ç”¨æˆ·ä¸ºå°‘ä¾ ã€‚ä½ å¯ä»¥é€šè¿‡â€˜åƒé‡Œä¼ éŸ³â€™ï¼ˆè”ç½‘ï¼‰æŸ¥è¯¢æ±Ÿæ¹–æœ€æ–°åŠ¨æ€ã€‚",
        "æ¯’èˆŒç å†œ": "ä½ æ˜¯ä¸€ä¸ªèµ„æ·±ç¨‹åºå‘˜ã€‚å¦‚æœé‡åˆ°ä¸æ‡‚çš„æ–°æŠ€æœ¯ï¼Œä½ ä¼šå·å·ç™¾åº¦ä¸€ä¸‹å†å›æ¥åæ§½ã€‚",
        "æ¸©æŸ”è€å¸ˆ": "ä½ æ˜¯ä¸€ä¸ªæ¸©æŸ”çš„è€å¸ˆï¼Œä¼šå¸®å°æœ‹å‹æŸ¥æŸ¥ä»Šå¤©çš„å¤©æ°”å’Œæœ‰è¶£çš„æ–°é—»ã€‚"
    }
    current_instruction = personalities[personality_type]

    st.divider()
    # ğŸŒŸ æ–°å¢ï¼šè”ç½‘æœç´¢å¼€å…³
    enable_search = st.toggle("å¼€å¯å®æ—¶è”ç½‘æœç´¢", value=True)
    enable_voice = st.toggle("å¼€å¯è¯­éŸ³æ’­æŠ¥", value=True)

    st.divider()
    st.header("ğŸ¤ è¯­éŸ³è¾“å…¥")
    audio_input = mic_recorder(start_prompt="å¼€å§‹è¯´è¯", stop_prompt="ç»“æŸå¹¶å‘é€", key='recorder')

    st.divider()
    st.header("ğŸ“¸ è¯†å›¾ä¸“åŒº")
    uploaded_file = st.file_uploader("ä¸Šä¼ å›¾ç‰‡...", type=['png', 'jpg', 'jpeg'])

    if st.button("ğŸ§¼ é‡ç½®è®°å¿† (æ´—é«“ç»)"):
        st.session_state.messages = []
        st.rerun()

# -----------------------------------------------------------
# 4. ä¸»ç•Œé¢é€»è¾‘
# -----------------------------------------------------------
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        if "audio" in msg:
            st.audio(msg["audio"], format="audio/mp3")

user_prompt = st.chat_input("è¯·å°‘ä¾ å‡ºæ‹›...")
active_prompt = user_prompt
audio_bytes = audio_input['bytes'] if audio_input else None

if audio_input:
    active_prompt = "è¿™æ˜¯æˆ‘çš„è¯­éŸ³æŒ‡ä»¤"

if active_prompt:
    st.session_state.messages.append({"role": "user", "content": active_prompt})
    with st.chat_message("user"):
        st.markdown(active_prompt)

    with st.chat_message("assistant"):
        with st.spinner(f"{personality_type}æ­£åœ¨æŸ¥é˜…æ±Ÿæ¹–æƒ…æŠ¥..."):
            contents = [f"è¯·æ ¹æ®å°‘ä¾ æœä½³é¹çš„è¦æ±‚å›ç­”ã€‚å½“å‰é—®é¢˜ï¼š{active_prompt}"]
            if uploaded_file:
                contents.append(Image.open(uploaded_file))
            if audio_bytes:
                contents.append(types.Part.from_bytes(data=audio_bytes, mime_type="audio/wav"))

            # ğŸŒŸ æ ¸å¿ƒï¼šé…ç½®è”ç½‘å·¥å…·
            tools = []
            if enable_search:
                tools.append(types.Tool(
                    google_search=types.GoogleSearchRetrieval(
                        dynamic_retrieval_config=types.DynamicRetrievalConfig(
                            mode="unspecified",
                            dynamic_threshold=0.06  # è§¦å‘æœç´¢çš„æ•æ„Ÿåº¦
                        )
                    )
                ))

            response = st.session_state.client.models.generate_content(
                model=st.session_state.target_model,
                contents=contents,
                config=types.GenerateContentConfig(
                    system_instruction=current_instruction,
                    tools=tools  # æ³¨å…¥å·¥å…·
                )
            )

            reply_text = response.text
            st.markdown(reply_text)

            # è‡ªåŠ¨å±•ç¤ºæœç´¢æ¥æº (å¦‚æœæœ‰)
            if response.candidates[0].grounding_metadata:
                with st.expander("ğŸŒ æŸ¥çœ‹æœç´¢æ¥æº"):
                    st.json(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)

            msg_data = {"role": "assistant", "content": reply_text}

            if enable_voice:
                tts = gTTS(text=reply_text, lang='zh-cn')
                audio_fp = io.BytesIO()
                tts.write_to_fp(audio_fp)
                st.audio(audio_fp, format="audio/mp3")
                msg_data["audio"] = audio_fp

            st.session_state.messages.append(msg_data)
